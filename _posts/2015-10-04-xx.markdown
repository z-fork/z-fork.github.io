---
title: TCP
layout: post
tags:
- tcp
---

### Intro

##### header
~~~ nohighlight
+---------------------------+---------------------------+----------------
|           src port        |           dst port        |   ^         ^
+---------------------------+---------------------------+
|                       seq number                      |
+---------------------------+---------------------------+
|                       ack number                      | 20 Bytes
+---------------------------+---------------------------+
| offset | reserved | flags |           window          |          offset
+---------------------------+---------------------------+
|           checksum        |       urgent pointer      |   v
+---------------------------+---------------------------+  ---
|                       tcp options                     |             v
+---------------------------+---------------------------+----------------

* TCP 在网络 OSI 的七层模型中
    TCP             - 第四层 Transport层    -- Segment  ( src_port, dst_port )
    IP              - 第三层 Network层      -- Packet   ( src_ip, dst_ip )
    Ethernet / ARP  - 第二层 Data Link层    -- Frame

* TCP Header
    src_port, dst_port; + { IP Header: src_ip, dst_ip }     - 表示唯一 TCP/IP 连接
    seq number                                              - 网络包乱序
    ack number                                              - 丢包
    adv-window                                              - 流控
    checksum { checksum(IP Header + TCP Header) }           -
    flag                                                    - 操控状态机
      { C: CWR, E: ECE, U: Urgent, A: Ack, P: Push, R: Reset, S: Syn, F: Fin }
    Options
        0: End of Options List
        1: Nop
        2: MSS( Maximum segment size )
        3: Window Scale
        4: Sack ( Selective ACK ok )
        8: Timestamp
~~~

##### 性质
~~~ nohighlight
1. 面向连接的, 可靠 ( seq 不乱序, ack 应答, 超时无应答则 retry 等机制 )
2. 有流量控制       ( slide window )
3. 拥塞控制         ( 慢启动, 拥塞窗口, 快速重传, 快速恢复 )
4. 全双工通信
5. 面向字节流, 点对点( 一对一 )
~~~

##### 状态
~~~ nohighlight
SYN_RECV, ESTABLISHED, TIME_WAIT

* 主动关闭
    发送:FIN -> FIN_WAIT_1
    +- 接收:ACK -> FIN_WAIT_2；                     接收:FIN、发送:ACK
    +- 接收:FIN、发送: ACK -> CLOSING( 同时关闭 )   接收:ACK
    +- 接收:FIN-ACK、发送: ACK
        +- TIME_WAIT(2MSL超时)
            +- CLOSED
* 被动关闭
    接收:FIN、发送: ACK -> CLOSE_WAIT
    +- 发送:FIN -> LAST_ACK
        +- 接收:ACK -> CLOSED

操作 + Flag + 状态 + value
    关闭
        { close / shutdown } -> { FIN / RST } -> { TIME_WAIT( 2msl ) / CLOSE_WAIT }
~~~

### keyword

##### value
~~~ nohighlight
MTU, MSS, MSL, TTL, RTO, RTT, BDP
~~~

##### params
~~~ nohighlight
tcp_max_syn_backlog, somaxconn, tcp_abort_on_overflow, tcp_syncookies, tcp_defer_accept, tcp_fastopen

tcp_adv_win_scale, tcp_moderate_rcvbuf, nagle, tcp_sack, tcp_ecn, tcp_slow_start_after_idle

so_linger, tcp_max_tw_buckets, tcp_timestamps, tcp_tw_reuse, tcp_tw_recycle

tcp_synack_retries, tcp_retries1, tcp_fin_timeout

tcp_rmem, tcp_wmem, tcp_mem, rmem_default, wmem_default, rmem_max, wmem_max

tcp_orphan_retries, tcp_max_orphans
~~~

##### methods
~~~ nohighlight
关闭: close, shutdown
发送: tcp_sendmsg, tcp_write_queue, tcp_push
接收: tcp_v4_rcv
~~~

##### construct:
~~~ nohighlight
队列:
    syn 队列, accept 队列
    receive 队列, out_of_order 队列, backlog 队列, prequeue 队列
滑动窗口
拥塞窗口 { 慢启动、拥塞避免、快速重传、快速恢复 }
~~~

### interpret

##### MTU - 最大传输单元

MTU 作用在数据链路层(Data Link, Frame size), TCP 处于链路层之上, 所以也受限于 MTU 大小限制
~~~
    以太网  1500字节
    802.3   1492字节
~~~
报文中 IP 头部 16bit 表示 Packet 大小, 2^16-1 = 65535字节

TCP 层在发送一个大于 1500 字节的消息, 调用 IP 层方法发送时, IP 层会自动的获取所在网络的 MTU, 并按照所在网络的 MTU 大小来分片,
会被分成若干个小于 MTU 的 Packet, 每个 Packet 都会有**独立的 IP Header**.

IP 层希望**分片**对于 TCP 层来说是**透明**的, 接收方的 IP 层会根据收到的多个 IP 包头部, 将分片的 Packet 重组为一个 Segment
~~~
IP层的分片效率是很差
    因为必须所有分片都到达才能重组成一个包
    任何一个分片丢失了, 都必须重发所有分片
~~~

##### MSS - 最大 Segment 段长度

MSS 1460 = 1500 - 20 - 20；以太网的 MTU 为 1500, 减去 IP 和 TCP Header 的长度

TCP 层会试图避免 IP 层执行数据报分片, TCP 协议定义了 MSS, 建立连接时, 连接双方都要互相告知自己期望接收到的 MSS 大小,
会根据对方告知的 MSS 来分片, 把消息分为多个 Segment, 再调用 IP 层方法来发送

在建立握手时告知对方期望接收的 MSS 值是预估的, 连接上可能有许多中间网络, 分别具有不同的数据链路层( 以太网, 802.3, ... MTU 都不同 )
通过 { IP头部的 DF标志位 }, 这个标志位是不要对 Packet 分片, Packet 需要分片则返回 ICMP错误 + 可接受的 MTU 值, 发送方就重新确定 MSS

##### MSL ( Maximum Segment Lifetime )

它定义了一个 Segment 在网络中的最长生存时间, Linux 系统中, MSL 的值固定为 30 秒

`2MSL - TIME_WAIT`
* 防止收到历史数据, 从而导致数据错乱的问题
    ~~~
    a. 旧的数据包被网络延迟, 通过 retry 的数据包完成整个逻辑
    b. TCP 相同端口号连接重新建立, 旧的数据包到来, 发生错乱
    ~~~
* 等待足够的时间以确保最后的 ACK 能让被动关闭方接收, 从而帮助其正常关闭
    ~~~
    a. 当主动关闭端 FIN_WAIT_2 -> (发送 ACK) TIME_WAIT -> CLOSED；TIME_WAIT 过短/没有 ACK 丢失
    b. TCP 相同端口号连接重新建立, 被动关闭端还处在 LAST_ACK, 导致直接 RST
    ~~~

##### TTL

报文每经过一次路由转发, IP 头部的 TTL 字段就会减1, 减到 0 时报文就被丢弃, 从而限制了报文的最长存活时间

##### RTO

重试间隔? TODO: ...

##### RTT

网络延时

##### BDP - 带宽时延积( Bandwidth Delay Product )

最大带宽是 100 MB/s, 网络时延( RTT )是 10ms 时, 意味着客户端到服务端的网络一共可以存放 100MB/s * 0.01s = 1MB 的字节

这个 1MB 是带宽和时延的乘积叫 BDP, 同时, 这 1MB 也表示「飞行中」的 TCP 报文大小, 它们就在网络线路、路由器等网络设备中

### params

~~~nohighlight
/proc/sys/net/ipv4/tcp_max_syn_backlog
/proc/sys/net/core/somaxconn
/proc/sys/net/ipv4/tcp_syncookies               # 0 关闭该功能; 2 无条件开启功能; 1 仅当 SYN 半连接队列放不下时, 再启用它
/proc/sys/net/ipv4/tcp_abort_on_overflow        # 0 表示关闭该功能; 1 表示开启功能, 返回 RST
/proc/sys/net/ipv4/tcp_fastopen                 # 0 关闭; 1 C 端使用 FastOpen; 2 S 端使用 FastOpen; 3 C/S 端都使用 FastOpen

net.ipv4.tcp_adv_win_scale                      # 滑动窗口比例 1 / 2^tcp_adv_win_scale
net.ipv4.tcp_moderate_rcvbuf = 1                # 1：表示打开了tcp内存自动调整功能; 0：这个功能将不会生效( 慎用 )

net.ipv4.tcp_slow_start_after_idle = 0          # 关闭tcp的连接传输的慢启动, 即先休止一段时间, 再初始化拥塞窗口
net.ipv4.tcp_ecn = 0                            # 把TCP的直接拥塞通告(tcp_ecn)关掉
net.ipv4.tcp_sack = 1                           # 启用 sack 通过有选择地应答乱序接收到的报文来提高性能

/proc/sys/net/ipv4/tcp_max_tw_buckets
/proc/sys/net/ipv4/tcp_timestamps               # 1 打开时间戳, 默认值 1
/proc/sys/net/ipv4/tcp_tw_reuse                 # 1 打开 tcp_tw_reuse 功能
/proc/sys/net/ipv4/tcp_tw_recycle               # deprecated on Linux 4.12

/proc/sys/net/core/wmem_default                 # tcp连接分配内核缓存

net.ipv4.tcp_rmem = 8192 87380 16777216
net.ipv4.tcp_wmem = 8192 65536 16777216
net.ipv4.tcp_mem = 8388608 12582912 16777216

net.ipv4.ip_local_port_range = 1024 65000       # 表示用于向外连接的端口范围缺省情况下很小: 32768到61000, 改为1024到65000
net.ipv4.netfilter.ip_conntrack_max=204800      # 设置系统对最大跟踪的TCP连接数的限制
net.ipv4.route.gc_timeout = 100                 # 路由缓存刷新频率, 当一个路由失败后多长时间跳到另一个路由, 默认是300
net.core.netdev_max_backlog                     #
~~~

##### tcp_max_syn_backlog

服务端建立连接时, SYN_RECV (SYN 队列) -> ESTABLISHED (ACCEPT 队列) 过程中, SYN_RECV 所处状态为"半连接"
deprecated! 早的内核控制 SYN 队列, 参考 source 1 与实际内核版本, 关注 goto drop & drop_and_free 情景

##### somaxconn               { exp-3 }

服务端建立连接时, SYN_RECV (SYN 队列) -> ESTABLISHED (ACCEPT 队列) 过程中, ESTABLISHED 所处状态为"全连接"
listen backlog (数值上相关, 并不是相等, 具体看内核对应处理)

##### tcp_syncookies          { exp-4  }      - syn 队列塞满

正常情况下, SYN 队列大小与 ACCEPT 队列大小相关, 在 SYN flood 攻击等, 非正常占满 SYN 队列情况下, 开启 tcp_syncookies 不影响正常请求建立连接.
服务端建立连接时, SYN 队列已满, 启用 tcp_syncookies 机制后, 不会 drop/abort 而是计算一个值随 SYN+ACK 返回, 当 cli 返回 ACK 报文时, 取值验证来建立连接
* 这种方式建立的连接, 许多 TCP 特性都无法使用 ( 少了 syn 过程, 依赖 seq number 的流控, 丢包重传都无效了)

##### tcp_defer_accept        { exp-5 }       - accept 队列塞满
客户端设置后, 握手过程中 ack + DATA 合并发送 tcpdump 中 [P.]
服务端设置后, 忽略握手 ack, 延迟到有 DATA 才会接收, 在一定程度上缓解了 TCP 全连接攻击, 避免无数据 conn 入 ACCEPT 队列
* accept() 接受请求后, 创建相关数据结构, 比如 ngx_connection_t, 如果这个请求迟迟不结束, ngx_connection_t 不能及时释放, 后续再来正常的客户端请求提供服务
* 相比在应用层处理这些非法的客户端请求( 即TCP全连接攻击 )所带来的消耗, 把它们直接交给内核去做处理所消耗的系统资源会更少一点

##### tcp_abort_on_overflow   { exp-6 }       - 是否 rst
当 ACCEPT 队列满时, 1. 丢弃连接只是 Linux 的默认行为; 2. 可以选择向客户端发送 RST 复位报文, 告诉客户端连接建立失败.
* 默认选择丢弃, 更有利通过重试能适当应急突发流量, 客户端行为 wait RTO * 2^times && retry 这种方式尽可能处理请求

##### tcp_fastopen                            - 减少3次握手

第一次发起请求的时候, 还是需要正常的三次握手流程
~~~
1. 客户端发送 SYN 报文, 该报文包含 Fast Open 选项, 且该选项的 Cookie 为空, 这表明客户端请求 Fast Open Cookie；
2. 支持 TCP Fast Open 的服务器生成 Cookie, 并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；
3. 客户端收到 SYN-ACK 后, 本地缓存 Fast Open 选项中的 Cookie
~~~
之后, 如果客户端再次向服务器建立连接时的过程：
~~~
1. 客户端发送 SYN 报文, 该报文包含 Cookie + DATA
2. 服务端收到 Cookie 进行校验：
    如果 Cookie 有效, 将在 SYN+Cookie+DATA 报文中对 SYN 和 DATA 进行确认, 服务器随后将 DATA 递送至相应的应用程序；
    如果 Cookie 无效, 将丢弃 SYN 报文中包含的 DATA, 且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；
3. 客户端收到 SYN-ACK 进行判断
    如果服务端在初始的 SYN+Cookie+DATA 报文中 DATA 没有被确认, 与一般三次握手一样只确认 SYN 的对应序列号,
    则客户端将重新发送 DATA；
~~~
* 如果服务器接受了 SYN 报文中的 DATA, 可在握手完成之前发送数据, 这就减少了握手带来的 1 个 RTT 的时间消耗；

##### tcp_adv_win_scale

读缓存 = 应用程序读缓存 + 接收滑动窗口
当 tcp_adv_win_scale 为2时, 1/(2^tcp_adv_win_scale)=1/4 做应用读缓存, 最大的接收滑动窗口的大小只能到达 3/4
读缓存的作用有2个：
~~~
1、将无序的、落在接收滑动窗口内的TCP报文缓存起来；
2、当有序的、可以供应用程序读取的报文出现时, 由于应用程序的读取是延时的, 所以会把待应用程序读取的报文也缓存起来
~~~
所以, 读缓存一分为二, 一部分缓存无序报文, 一部分缓存待延时读取的有序报文

当应用程序读取速率过慢时, 使接收滑动窗口缩小, 从而通知连接的对端降低发送速度, 避免无谓的网络传输
当应用程序长时间不读取数据, 造成应用缓存将套接字缓存挤压到没空间, 那么连接对端会收到接收窗口为 0 的通知,

初始的接收窗口是5792, 远小于最大接收缓存 - 慢启动( 拥塞窗口 )

##### tcp_moderate_rcvbuf
linux为了实现 a, b场景, 引入了自动调整内存分配的功能, 由 tcp_moderate_rcvbuf 配置决定
~~~
a. 在并发连接比较少时, 把缓存限制放大一些, 让每一个TCP连接开足马力工作；
b. 当并发连接很多时, 此时系统内存资源不足, 那么就把缓存限制缩小一些, 使每一个TCP连接的缓存尽量的小一些, 以容纳更多的连接
~~~

##### so_linger & l_linger
如果 close 时发出的消息丢失, 进程退出时 RST, 且之前丢失的消息没有 retry 来保障可靠性
* 尽可能能收到 FIN 的 ACK 确保最后的响应对方收到, 阻塞 close 一个最大等待时间 l_linger
* l_linger = 0 直接 RST 关闭

##### tcp_max_tw_buckets
TIME_WAIT 状态过多, 会占用 1. 比较紧缺的 port 资源; 2. 在 accept & fork(clone) 模型, 进程/线程等系统资源

TIME_WAIT 的连接数量超过该参数时, 新关闭的连接就不再经历 TIME_WAIT 而直接关闭

##### tcp_tw_reuse & tcp_timestamps
tcp_max_tw_buckets 只是限制量, 并不是通过一种机制、协议、约定来避免数据错乱、非正常关闭情况; 通过 timestamps 有序, 类似形成有状态的延续
启用 tcp_timestamps, 打开 tcp_tw_reuse 参数, 复用 TIME_WAIT 状态的连接, tcp_tw_reuse 影响 connect
适用范围
~~~
只适用于连接发起方, 也就是 C/S 模型中的 client; 2. 对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用
因为重复的数据包会因为时间戳过期被自然丢弃; 2. 它还可以防止序列号绕回, 也是因为重复的数据包会由于时间戳过期被自然丢弃
~~~

##### tcp_tw_recycle
~~~
1. 快客户端和服务端 TIME_WAIT 状态的时间, 也就是它会使得 TIME_WAIT 状态会小于 60 秒, 很容易导致数据错乱；
2. Linux 会丢弃所有来自远端时间戳小于上次记录的时间戳( 由同一个远端发送的 )的任何数据包
* 使用该选项, 则必须保证数据包的时间戳是单调递增的
    此处的时间戳并不是我们通常意义上面的绝对时间, 而是一个相对时间, 很多情况下, 我们是没法保证时间戳单调递增的, 比如使用了 NAT、LVS 等情况；
~~~

##### tcp_rmem, tcp_wmem, tcp_mem

tcp_rmem[3]数组表示任何一个TCP连接上的读缓存上限,

    tcp_rmem[0]表示最小上限; tcp_rmem[1]表示初始上限; tcp_rmem[2]表示最大上限

tcp_wmem[3]数组表示写缓存, 与tcp_rmem[3]类似

tcp_mem[3]数组就用来设定TCP内存的整体使用状况, 所以它的值很大( 它的单位也不是字节, 而是页--4K或者8K等这样的单位！ )

    TCP整体内存的无压力值、压力模式开启阀值、最大使用值
    1、当TCP整体内存小于tcp_mem[0]时, 表示系统内存总体无压力
        若之前内存曾经超过了tcp_mem[1]使系统进入内存压力模式, 那么此时也会把压力模式关闭
        这种情况下, 只要TCP连接使用的缓存没有达到上限( 初始上限是tcp_rmem[1], 但这个值是可变的 ), 那么新内存的分配一定是成功
    2、当TCP内存在tcp_mem[0]与tcp_mem[1]之间时, 系统可能处于内存压力模式,
        例如总内存刚从tcp_mem[1]之上下来；也可能是在非压力模式下, 例如总内存刚从tcp_mem[0]以下上来
        此时, 无论是否在压力模式下, 只要TCP连接所用缓存未超过tcp_rmem[0]或者tcp_wmem[0], 那么都一定都能成功分配新内存
        否则, 基本上就会面临分配失败的状况 ( 注意：还有一些例外场景允许分配内存成功, 由于理解这几个配置项意义不大, 故略过 )
    3、当TCP内存在tcp_mem[1]与tcp_mem[2]之间时, 系统一定处于系统压力模式下 其他行为与上同
    4、当TCP内存在tcp_mem[2]之上时, 毫无疑问, 系统一定在压力模式下, 而且此时所有的新TCP缓存分配都会失败
    当系统在非压力模式下, 上面我所说的每个连接的读写缓存上限, 才有可能增加, 当然最大也不会超过tcp_rmem[2]或者tcp_wmem[2] 相反,
    在压力模式下, 读写缓存上限则有可能减少, 虽然上限可能会小于tcp_rmem[0]或者tcp_wmem[0]

所以, 粗略的总结下, 对这3个数组可以这么看：

    1、只要系统TCP的总体内存超了 tcp_mem[2] , 新内存分配都会失败
    2、tcp_rmem[0]或者tcp_wmem[0]优先级也很高, 只要条件1不超限, 那么只要连接内存小于这两个值, 就保证新内存分配一定成功
    3、只要总体内存不超过tcp_mem[0], 那么新内存在不超过连接缓存的上限时也能保证分配成功
    4、tcp_mem[1]与tcp_mem[0]构成了开启、关闭内存压力模式的开关
        在压力模式下, 连接缓存上限可能会减少
        在非压力模式下, 连接缓存上限可能会增加, 最多增加到tcp_rmem[2]或者tcp_wmem[2]

##### close

tcp 本身为全双工, close 意味着关闭读写
* LISTEN sock
    移除 keepalive 定时器, SYN 队列中半连接 RST 关闭
* ESTABLISHED sock
    a. 若有收到还没处理消息( 已ACK, 未read ), RST 关闭
    b. 否则, 发送 FIN, 走正常 4 次挥手；因滑动窗口、拥塞窗口、angle 算法等顺续发完数据, 最后一个数据 + FIN / 单独发 FIN
* 多进程/多线程

    1. close 系统调用过程 sys_close -> filp_close -> fput -> ... -> sock_close ... inet_release -> tcp_close
        socket 也是作为 fd 形式存在, 也遵循 fd close, 判断 fd 引用计数( fput 函数中 )来进入后续真正关闭流程
    2. 多进程 clone(CLONE_FILES) -> copy_files -> dup_fd
        增加了 fd 的引用计数, 所以, 需要所有进程中关闭 socket fd 才真正关闭

##### shutdown

close 仅提供直接关闭读写, 因双工, 通过 shutdown 支持单向关闭
* LISTEN sock
    关闭读：同 close, 移除 keepalive 定时器, SYN 队列中半连接 RST 关闭
    关闭写：无意义
* ESTABLISHED sock
    关闭读：仅把收到的消息丢弃, 记标记, 并不要求对方关闭写；read 等方法读不到任何数据
    关闭写：同 close, RST or FIN 来关闭
* 多进程/多线程
    1. shutdown 系统调用过程 sys_shutdown -> inet_shutdown -> tcp_shutdown
        直接对应 socket, 不走文件系统 fd 那套

##### tcp_sendmsg

~~~
把待发送数据, 按照 MSS 来划分成多个报文段, 以 sk_buff 结构放到这个TCP连接对应的 tcp_write_queue 发送队列中

tcp_sendmsg { sk_stream_wait_memory -> tcp_sendmsg }
    +- tcp_push{tcp_cwnd_test -> tcp_snd_wnd_test -> tcp_nagle_test -> tcp_window_allows -> tcp_transmit_skb}

sk_stream_wait_memory
    TCP 连接分配的内核缓存是有限的, 当没有缓存来复制待发送数据时, 调用此方法来等待滑动窗口移动, 释放出一些缓存出来
    ( 收到ACK后, 不需要再缓存原来已经发送出的报文, 因为已经确认对方收到, 就不需要定时重发, 自然就释放缓存了 )

    wait_for_memory:
        if (copied)
            tcp_push(sk, tp, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
        if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
            goto do_error;
    ...
    timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
    static inline long sock_sndtimeo(const struct sock *sk, int noblock)
    {
        return noblock ? 0 : sk->sk_sndtimeo;
    }
    * 对于 non-blocking 会直接返回, 并将 errno 为 EAGAIN
~~~

##### tcp_push

~~~
tcp_sendmsg{sk_stream_wait_memory -> tcp_sendmsg}
    +- tcp_push{tcp_cwnd_test -> tcp_snd_wnd_test -> tcp_nagle_test -> tcp_window_allows -> tcp_transmit_skb}

tcp_cwnd_test - 拥塞窗口
    在 tcp_push发送消息时, 检查拥塞窗口, 飞行中的报文数要小于拥塞窗口个数( 多少个MSS的个数 )

    cwnd 就是拥塞窗口, 它用来帮助慢启动的实现

    慢启动算法：
        握手时对方通告的窗口大小只表示对方接收TCP分组的能力, 不表示中间网络能够处理分组的能力
        所以, 发送方请悠着点发, 确保网络非常通畅了后, 再按照对方通告窗口来发

    连接刚建立时, 它实际上是一个MSS 每收到一个ACK, 拥塞窗口扩大一个MSS大小, 最大只能到对方通告的接收窗口大小
    当然, 为了避免指数式增长, 拥塞窗口大小的增长会更慢一些, 是线性的平滑的增长过程

tcp_snd_wnd_test - 滑动窗口限制
    待发送的最大序号 ~ 已经发送最小的没被确认的序号 + 发送窗口大小

tcp_nagle_test
    Nagle 算法
        要求一个TCP连接上最多只能有一个发送出去还没被确认的小分组, 在该分组的确认到达之前不能发送其他的小分组
    Nagle 算法的初衷是这样的：
        应用进程调用发送方法时, 可能每次只发送小块数据, 对于整个网络的执行效率来说, 小的TCP报文会增加网络拥塞的可能,
        因此, 如果有可能, 应该将相临的TCP报文合并成一个较大的TCP报文( 当然还是小于MSS的 )发送

tcp_window_allows
    获取拥塞窗口与滑动窗口的最小长度, 检查待发送的数据是否超出：

* 调用了IP层的方法返回后, 也未必就保证此时数据一定发送到网络中去了
~~~

##### tcp_v4_rcv

~~~
* 使用阻塞socket, 调用recv等方法时flag标志位为0, 读取 socket 时没有发生进程睡眠

当网卡接收到报文并判断为TCP协议后, 将会调用到内核的 tcp_v4_rcv 方法
    |
    |- 需要接收的报文序号是S1, 网卡上收到了S1-S2的报文, 这个报文直接插入到 receive 队列中
    |- 需要接收的报文序号是S2, 但到来的报文是S3, 进入out_of_order 队列, 所有乱序的报文都会暂时放在这
        |
        |- 仍然没有进程来读取socket, 又来了我们期望的S2-S3报文, 直接进入receive队列 此时 out_of_order 队列非空
            |
            |- 每次 receive 队列插入报文时都会检查 out_of_order 队列 由于收到S2报文后, 期待的序号成为了S3,
                这样, out_of_order 队列里的报文S3报文将会移出本队列而插入到 receive 队列中( 由 tcp_ofo_queue 方法 )
                |
                |- 用户进程读取 socket 了, 先分配一块内存, 调用read或者recv等方法, 这个socket还可以配置其属性
                    这里假定没有设置任何属性, 都使用默认值, 因此, 此时socket是阻塞式, 它的SO_RCVLOWAT是默认的1
                    当然, recv这样的方法还会接收一个flag参数, 它可以设置为MSG_WAITALL、MSG_PEEK、MSG_TRUNC 等等,
                    这里我们假定为最常用的0 进程调用了recv方法
                    |
|-------------------|
|
|- 经过层层封装, 接收TCP消息最终一定会走到 tcp_recvmsg 方法
    |
    |- lock socket；socket是可以被多进程同时使用的, 内核也会操作它,
        而下面的代码都是核心的、操作数据的、有状态的代码, 不可以被重入的；
        再有用户进程进来时拿不到锁就要休眠在这了 内核中断看到被锁住后也会做不同的处理
        |
        |- 此时 receive 队列准备好了3个报文 最上面的报文是S1-S2, 将它拷贝到用户态内存中
            由于 flag 参数并没有携带 MSG_PEEK 这样的标志位, 因此, 再将S1-S2报文从 receive 队列的头部移除, 从内核态释放
            反之, MSG_PEEK 标志位会导致 receive 队列不会删除报文 所以, MSG_PEEK 主要用于多进程读取同一套接字的情形
            |
            |- 拷贝S2-S3报文到用户态内存中 当然, 执行拷贝前都会检查用户态内存的剩余空间是否足以放下当前这个报文,
                不足以时会直接返回已经拷贝的字节数
                |
                |- S3-S4 同上
                    |
                    |- receive 队列空了, 此时会先来检查 SO_RCVLOWAT 这个阀值
                        如果已经拷贝的字节数到现在还小于它, 那么可能导致进程会休眠, 等待拷贝更多的数据
                        socket 使用的默认的SO_RCVLOWAT, 也就是1, 这表明, 只要读取到报文了, 就认为可以返回了
                        再检查backlog 队列 backlog 队列是进程正在拷贝数据时, 网卡收到的报文会进这个队列
                        |
                        |- backlog 队列是没有数据的, 已经拷贝的字节数大于1的, 因此, 释放第7步里加的锁, 准备返回用户态
                        |
|-----------------------|
|
|- 用户进程代码开始执行, 此时recv等方法返回的就是S4-S1, 即从内核拷贝的字节数

* 用户进程调用recv方法时, 连接上没有任何接收 & 缓存到内核的报文, 而socket是阻塞的, 所以进程睡眠了

最终调用到 tcp_recvmsg 方法来处理
    |
    |- lock socket；同上
        |
        |- 由于此时 receive 队列、prequeue 队列、backlog 队列都是空的, 即没有拷贝1个字节的消息到用户内存中,
            而我们的最低要求是拷贝至少 SO_RCVLOWAT 为1长度的消息 此时, 开始进入阻塞式套接字的等待流程
            最长等待时间为 SO_RCVTIMEO 指定的时间
            睡眠前会调用 sk_wait_data 执行 release_sock, 释放socket锁, 使得新到的报文不再只能进入 backlog 队列
            |
|---------|
|
|- 当网卡接收到报文并判断为TCP协议后, 将会调用到内核的 tcp_v4_rcv 方法
    |
    |- 需要接收的报文序号是S1, 网卡上收到了S1-S2的报文, 通过调用 tcp_prequeue 方法把报文插入到 prequeue 队列中
        |
        |-插入 prequeue队列后, 此时会接着调用 wake_up_interruptible 方法, 唤醒在socket上睡眠的进程
            |
            |- 用户进程被唤醒后, 重新调用lock_sock接管了这个socket, 此后再进来的报文都只能进入backlog队列了
                |
                |- 进程醒来后, 先去检查 receive队列, 当然仍然是空的；
                    再去检查 prequeue队列, 发现有一个报文S1-S2, 正好是socket连接待拷贝的起始序号S1,
                    从 prequeue队列中取出这个报文并把内容复制到用户内存中, 再释放内核中的这个报文
                    |
                    |- 目前已经拷贝了S2-S1个字节到用户态, 检查这个长度是否超过了最低阀值( SO_RCVLOWAT的最小值 )
                        |
                        |- 由于SO_RCVLOWAT使用了默认的1, 所以准备返回用户
                            此时会顺带再看看 backlog队列, 检查这个无序的队列中是否有可以直接拷贝给用户的报文, 此时是没有
                            准备返回, 释放socket锁
                            |
                            |- 返回用户已经拷贝的字节数

* 我们把系统参数 tcp_low_latency 设为1, socket上设置了SO_RCVLOWAT属性的值 服务器先是收到了S1-S2这个报文,
    但S2-S1的长度是小于SO_RCVLOWAT的, 用户进程调用recv方法时, 虽然读到了一些, 但没有达到最小阀值, 所以进程睡眠了,
    与此同时, 在睡眠前收到的乱序的S3-S4包直接进入backlog队列
    此时先到达了S2-S3包, 由于没有使用prequeue队列, 而它起始序号正是下一个待拷贝的值, 所以直接拷贝到用户内存中,
    总共拷贝字节数已满足SO_RCVLOWAT的要求！最后在返回用户前把backlog队列中S3-S4报文也拷贝给用户了

当网卡接收到报文并判断为TCP协议后, 将会调用到内核的 tcp_v4_rcv 方法
    |
    |- 需要接收的报文序号是S1, 网卡上收到了S1-S2的报文, 这个报文直接插入到 receive 队列中
        |
        |- 用户进程调用了recv方法接收socket上的消息, socket上设置了SO_RCVLOWAT属性为某个值n, 这个n是大于S2-S1,
|-----------|
|
|- 经过层层封装, 接收TCP消息最终一定会走到 tcp_recvmsg 方法
    |
    |- lock socket；
        |
        |- receive队列中的有序报文可直接拷贝, 在检查到S2-S1是小于len之后, 将报文内容拷贝到用户态内存中
            |
            |- 同时socket是被锁住的, 这时又收到了一个S3-S4报文, 因此进入backlog队列
                注意, 这个报文不是有序的, 因为此时连接上期待接收序号为S2
                |
                |- 拷贝了S2-S1个字节到用户内存, 它是小于SO_RCVLOWAT的, 由于socket是阻塞型套接字( 超时时间在本文中忽略 ),
                    进程将不得不转入睡眠 转入睡眠之前, 还会干一件事, 就是处理 backlog队列里的报文,
                    睡眠前会调用 sk_wait_data 执行 release_sock, 同 case 2, 额外, 此时 backlog 非空
                        |
                        |- 遍历 backlog队列, S3-S4报文因为它是失序的, 所以从 backlog队列中移入out_of_order队列中
                            |
                            |- 进程休眠, 直到超时或者receive队列不为空
                                |
+-----------------------------+
|
|- 内核接收到了S2-S3报文 注意, 这里由于打开了tcp_low_latency标志位, 这个报文是不会进入 prequeue队列
    |
    +- 由于S2是连接上正要接收的序号, 同时, 有一个用户进程正在休眠等待接收数据中,
        且它要等待的数据起始序号正是S2, 于是, 使得网络软中断执行上下文中, 把S2-S3报文直接拷贝进用户内存
        |
        +- 每处理完1个有序报文( 无论是拷贝到receive队列还是直接复制到用户内存 )后都会检查out_of_order队列,
            看看是否有报文可以处理 S3-S4报文恰好是待处理的, 于是拷贝进用户内存 然后唤醒用户进程
            |
            +- 用户进程被唤醒了, 当然唤醒后会先来拿到socket锁 以下执行又在进程上下文中了
                |
                +- 此时会检查已拷贝的字节数是否大于SO_RCVLOWAT, 以及backlog队列是否为空, 两者皆满足, 准备返回, 释放socket锁
                    |
                    +- 返回用户已经复制的字节数S4-S1

tcp_v4_rcv
    +- tcp_prequeue
        +- tcp_low_latency: 0；进入 prequeue 队列
        |   +- Done
        +- tcp_low_latency: 1；
            +- tcp_v4_do_rcv
                +- tcp_rcv_established
                    +- tcp_data_queue
                        +- tcp_ofo_queue

tcp_recvmsg
    |
    |- sock_rcvtimeo & nonblock
    |- sock_rcvlowat(sk, flags & MSG_WAITALL, len)  # 用户态内存大小len 和 SO_RCVLOWAT 的最小值
    |- if (!(flags & MSG_TRUNC))                    # MSG_TRUNC 标志位表示不要管len这个用户态内存有多大, 只管拷贝数据
    |- sk_wait_data
    |- release_sock

tcp_low_latency
    1：服务器希望TCP进程能够更及时的接收到TCP消息

receive 队列
    允许用户进程直接读取的, 它是将已经接收到的TCP报文, 去除了TCP头部、排好序放入的、用户进程可以直接按序读取
~~~

##### 拥塞窗口

* 慢启动

    ~~~
    使初始窗口尽量的小, 随着接收有效报文确认网络负载, 才开始增大接收窗口
    不同的linux内核有着不同的初始窗口,
        linux2.6.18内核
            在以太网里, MSS大小为1460, 初始窗口大小为 4*MSS
            1460 = 1500(MTU) - 20(IP Header) - 20(TCP Header)
            5792 = (1460 - 12(timestamp)) * 4
        linux3以后的版本中,
            初始窗口调整到了 10*MSS, 这主要来自于GOOGLE的建议
    ~~~

### Q

~~~ nohighlight
Q: TCP如何保证可靠传输；
    主要通过 checksum, seq number, ack, 超时重传, 流量控制, 拥塞控制 { 慢启动, 拥塞窗口, 快速重传, 快速恢复 }

Q: 3次握手; 1. 为什么是 3次?    2. 主要做了什么, 可以达到什么目的?                  { exp-1 }
    1. tcp 中使用 req / ack 模式, 被动方 ack/req 合并, 一共3次. 通过 ack 可达及重试防丢包
    2. 初始化 seq { ISN: initial sequence number }, 交换 header, 涉及
        a. 双方同步 seq { SYN: synchronize sequence number }, window    - tcp headers
        b. mss, sack, ecr, wscale, timestamp, ...                       - tcp options

Q: 4次挥手; 1. 为什么是 4次?    2. 主要做了什么, 可以达到什么目的?                  { exp-2 }
    1. 并不一定是 4次, 一方主动关闭, 另一方被动关闭时才是3-4次, 双方同步关闭是 2次.
    2. tcp 全双工, 收到 fin 时, 不能一定 ack / fin 合并返回, 因滑动窗口、拥塞窗口、angle 算法等顺序发送数据, 可能 push + fin, 也可能单独 fin

Q: 什么时候触发 RST ?
    close:
        server - (listen sock), syn 队列中的半连接 RST. ( accept 队列 发送 fin )
        client - (establised sock), 已 ack, 未 recv 读 RST. ( 否则, 正常4次挥手, 因 slide win, 拥塞, angle算法等可能 push + fin / fin )
    shutdown:
        server - shutdown read: 同 close server
        client - shutdown write: 同 close client
    param:
        tcp_abort_on_overflow, accept 队列满时 RST.

Q: 最大读缓存到底应该设置到多少为合适呢？
    BDP ~ mem + tcp_adv_win_scale
    当窗口过大时, 对于网络负载来说过大, 就会对网络设备引发恶性循环, 不断的造成丢包; 而窗口过小时, 就无法充分的利用网络资源
    BDP 就表示了网络承载能力, 最大接收窗口就表示了可以不经确认发出的报文

Q: TCP拥塞中的慢重传算法；
    慢开始算法的思路就是, 不要一开始就发送大量的数据, 先探测一下网络的拥塞程度, 也就是说由小到大逐渐增加拥塞窗口的大小

Q: close_wait, time_wait状态原因
    close_wait: 被动关闭, ( 可以recv EOF, 这时需要主动 close conn, 当然 recv后处理过长, cli 超时close, 处理过后write 会 RST 使其close )
    time_wait:  主动关闭, fin -> ack,fin -> ack 之后会保持 time_wait 状态 2MSL, 对丢包的 retry 等情况可以正常处理
    * 服务端处理过长且对 conn 未及时关闭 close_wait 过多 ( 异常现象, 需要 fix bugs )
    * http/1.x not keepalive 服务端主动关闭, time_wait 过多 ( 正常现象, 通过 timestamp + reuse 等方式优化 )

Q: 为什么建立连接协议是三次握手, 而关闭连接却是四次握手呢？
Q: 为什么 TIME_WAIT 状态还需要等 2MSL 后才能返回到 CLOSED 状态？

Q: 对于监听 socket 执行关闭, 和对处于 ESTABLISH 这种通讯的 socket 执行关闭, 有何区别？
Q: 当 socket 被多进程或者多线程共享时, 关闭连接时有何区别？

Q: 关连接时, 若连接上有来自对端的还未处理的消息, 会怎么处理？               (rst)
Q: 关连接时, 若连接上有本进程待发送却未来得及发送出的消息, 又会怎么处理？
Q: so_linger 这个功能的用处在哪？

Q: 发送方法成功返回时, 能保证TCP另一端的主机接收到吗？
Q: 能保证数据已经发送到网络上了吗？
Q: 套接字为阻塞或者非阻塞时, 发送方法做的事情有何不同？

Q: 应用程序调用read、recv等方法时, socket 可以设置为阻塞或者非阻塞, 这两种方式是如何工作的？
Q: 若socket为默认的阻塞, 此时recv方法传入的len参数, 是表示超时( SO_RCVTIMEO )或者接收到len长度的消息, recv方法才会返回？
    而且, socket上可以设置一个属性叫做SO_RCVLOWAT, 它会与len产生什么样的交集, 又是决定recv等接收方法什么时候返回？
Q: 应用程序开始收取TCP消息, 与程序所在的机器网卡上接收到网络里发来的TCP消息, 这是两个独立的流程 它们之间是如何互相影响的？
    例如, 应用程序正在收取消息时, 内核通过网卡又在这条TCP连接上收到消息时, 究竟是如何处理的？
    若应用程序没有调用read或者recv时, 内核收到TCP连接上的消息后又是怎样处理的？
Q: recv这样的接收方法还可以传入各种flags, 例如MSG_WAITALL、MSG_PEEK、MSG_TRUNC 等等 它们是如何工作的？
Q: 1个socket套接字可能被多个进程在使用, 出现并发访问时, 内核是怎么处理这种状况的？
Q: linux的sysctl系统参数中, 有类似tcp_low_latency这样的开关, 默认为0或者配置为1时是如何影响TCP消息处理流程的？

Q: tcp_rmem[2] 和 rmem_max 似乎都跟接收缓存最大值有关, 但它们却可以不一致, 究竟有什么区别？
Q: tcp_wmem[1]和wmem_default似乎都表示发送缓存的默认值, 冲突了怎么办？
Q: 在用抓包软件抓到的syn握手包里, 为什么TCP接收窗口大小似乎与这些配置完全没关系？
Q: 缓存上限是什么？
Q: 缓存的大小与TCP的滑动窗口到底有什么关系？

Q: 滑动窗口缩小到0时, 怎么恢复?
Q: ECN, ECE
~~~

### case

~~~ nohighlight
TIME_WAIT - timestamp + reuse

    a. 四次挥手中的最后一次 ACK 在网络中丢失了, 服务端一直处于 LAST_ACK 状态；
    b. 客户端由于开启了 tcp_tw_reuse 功能, 客户端再次发起新连接的时候, 会复用超过 1 秒后的 time_wait 状态的连接
        但客户端新发的 SYN 包会被忽略( 由于时间戳 ), 因为服务端比较了客户端的上一个报文与 SYN 报文的时间戳,
        过期的报文就会被服务端丢弃；
    c. 服务端 FIN 报文迟迟没有收到四次挥手的最后一次 ACK, 于是超时重发了 FIN 报文给客户端；
    d. 处于 SYN_SENT 状态的客户端, 由于收到了 FIN 报文, 则会回 RST 给服务端, 于是服务端就离开了 LAST_ACK 状态；
    e. 最初的客户端 SYN 报文超时重发了(  1 秒钟后 ), 此时就与服务端能正确的三次握手了

    * 可以在复用了 time_wait 状态的 1 秒过后成功建立连接, 这 1 秒主要是花费在 SYN 包重传
~~~

### exp

##### exp-1

~~~
$ tcpdump -i any port 5001 -nn -S
    IP1.58308 > IP2.5001:  Flags [S], seq 1424909309, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 3841599958 ecr 0,sackOK,eol], length 0
    IP2.5001  > IP1.58308: Flags [S.], seq 1568863893, ack 1424909310, win 28960, options [mss 1460,sackOK,TS val 344115119 ecr 3841599958,nop,wscale 7], length 0
~~~

##### exp-2
~~~
$ tcpdump -i any port 5001 -nn -S
$ nc -l 5001 / $ nc 127.0.0.1 5001 / $ netstat -atn | grep 5001
~~~

##### exp-3
~~~
$ ss -ltn
    State   Recv-Q   Send-Q     Local Address:Port      Peer Address:Port  Process
    LISTEN  0        3                0.0.0.0:5001           0.0.0.0:*
    Recv-Q：当前 accept 队列的大小, 已三次握手并等待服务端 accept() 的 TCP 连接
    Send-Q：accept 队列最大长度, 上面监听 5001 端口的 TCP 服务, accept 队列的最大长度为 3
~~~

##### exp-4
~~~
$ netstat -s | grep 'SYNs to LISTEN'
    28642 SYNs to LISTEN sockets dropped
~~~

##### exp-5
~~~
// nginx.conf
    listen 80 deferred;
~~~

##### exp-6
~~~
$ netstat -s | grep 'overflowed'
    22589 times the listen queue of a socket overflowed
~~~
